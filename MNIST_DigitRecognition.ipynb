{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ### IMPORT NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch utility imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "#neural net imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import external libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ### SET DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ### GET DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to convert images to PyTorch tensors\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset with the specified transformation\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ### SPLIT MNIST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and validation dataset\n",
    "torch.manual_seed(44)\n",
    "train_size = int(0.8 * len(train_dataset)) \n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataloaders\n",
    "torch.manual_seed(44)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot sample images\n",
    "torch.manual_seed(44)\n",
    "# Create a figure to display the images\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "# Print the first few images in a row\n",
    "for i, (image, label) in enumerate(train_loader):\n",
    "    if i < 5:  # Print the first 5 samples\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(image[0].squeeze(), cmap='gray')\n",
    "        plt.title(f\"Label: {label.item()}\")\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        break  # Exit the loop after printing 5 samples\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 12000, 10000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5020, 0.5020, 0.5020,\n",
       "           1.0000, 1.0000, 1.0000, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.2510, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.5020, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 0.7490, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.7490, 1.0000, 1.0000, 1.0000, 0.7490, 1.0000, 0.7490,\n",
       "           0.2510, 1.0000, 1.0000, 1.0000, 0.5020, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.5020, 1.0000, 1.0000, 1.0000, 0.5020, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.7490, 1.0000, 1.0000, 1.0000, 0.7490, 0.5020, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 0.7490, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2510, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.7490, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 0.5020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2510, 0.7490, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 0.7490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.7490, 1.0000, 1.0000, 0.7490, 0.2510,\n",
       "           1.0000, 1.0000, 1.0000, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2510, 1.0000, 1.0000, 0.7490, 0.2510, 0.0000,\n",
       "           0.5020, 1.0000, 1.0000, 0.5020, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.7490, 1.0000, 1.0000, 0.5020, 0.0000, 0.0000,\n",
       "           0.5020, 1.0000, 1.0000, 0.5020, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.2510, 0.0000, 0.0000,\n",
       "           0.5020, 1.0000, 1.0000, 0.5020, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 1.0000, 1.0000, 0.7490, 0.0000, 0.0000, 0.0000,\n",
       "           0.5020, 1.0000, 1.0000, 0.5020, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.5020, 1.0000, 1.0000, 0.5020, 0.0000, 0.0000, 0.0000,\n",
       "           0.7490, 1.0000, 1.0000, 0.5020, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.7490, 1.0000, 1.0000, 0.0000, 0.0000, 0.2510, 0.7490,\n",
       "           1.0000, 1.0000, 0.7490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.5020, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 0.7490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.7490, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2510, 0.5020, 1.0000, 1.0000, 0.5020, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check dataset sample\n",
    "#Check the dataset sample\n",
    "image, label = train_dataset[2]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ### PREPARE DATALOADERS FOR TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_dataloader 750 batches of size 64\n",
      "Length of train_dataloader 188 batches of size 64\n",
      "Length of test_dataloader 157 batches of size 64\n"
     ]
    }
   ],
   "source": [
    "#Prepare DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Set Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Turn datasets into iterables\n",
    "training_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False )\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "#Lets check what we have created\n",
    "\n",
    "print(f\"Length of train_dataloader {len(training_dataloader)} batches of size {BATCH_SIZE}\")\n",
    "print(f\"Length of validation_dataloader {len(validation_dataloader)} batches of size {BATCH_SIZE}\")\n",
    "print(f\"Length of test_dataloader {len(test_dataloader)} batches of size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6. ### CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define custom DenseNet-121 model for 28x28 images\n",
    "class DenseNet121_28x28(nn.Module):\n",
    "    def __init__(self, num_classes=10, bias=True, dropout=0.2):\n",
    "        super(DenseNet121_28x28, self).__init__()\n",
    "        self.model = models.densenet121(weights=None)  # Use weights=\"DEFAULT\" for pretrained\n",
    "        self.model.features.conv0 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "        \n",
    "        # Custom classifier with dropout and hidden layer\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512, bias=bias),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes, bias=bias)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet121_28x28(\n",
      "  (model): DenseNet(\n",
      "    (features): Sequential(\n",
      "      (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu0): ReLU(inplace=True)\n",
      "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (denseblock1): _DenseBlock(\n",
      "        (denselayer1): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (transition1): _Transition(\n",
      "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (denseblock2): _DenseBlock(\n",
      "        (denselayer1): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer7): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer8): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer9): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer10): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer11): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer12): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (transition2): _Transition(\n",
      "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (denseblock3): _DenseBlock(\n",
      "        (denselayer1): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer7): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer8): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer9): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer10): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer11): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer12): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer13): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer14): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer15): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer16): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer17): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer18): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer19): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer20): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer21): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer22): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer23): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer24): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (transition3): _Transition(\n",
      "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (denseblock4): _DenseBlock(\n",
      "        (denselayer1): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer7): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer8): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer9): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer10): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer11): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer12): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer13): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer14): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer15): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer16): _DenseLayer(\n",
      "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "      (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "model = DenseNet121_28x28(num_classes=10, bias=True, dropout=0.2)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print model architecture (optional)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10]\n",
      "  Batch 10/750 - Loss: 0.1569\n",
      "  Batch 20/750 - Loss: 0.3049\n",
      "  Batch 30/750 - Loss: 0.1177\n",
      "  Batch 40/750 - Loss: 0.2660\n",
      "  Batch 50/750 - Loss: 0.0188\n",
      "  Batch 60/750 - Loss: 0.4133\n",
      "  Batch 70/750 - Loss: 0.0898\n",
      "  Batch 80/750 - Loss: 0.0258\n",
      "  Batch 90/750 - Loss: 0.1345\n",
      "  Batch 100/750 - Loss: 0.0911\n",
      "  Batch 110/750 - Loss: 0.0151\n",
      "  Batch 120/750 - Loss: 0.1174\n",
      "  Batch 130/750 - Loss: 0.1744\n",
      "  Batch 140/750 - Loss: 0.0739\n",
      "  Batch 150/750 - Loss: 0.0317\n",
      "  Batch 160/750 - Loss: 0.0495\n",
      "  Batch 170/750 - Loss: 0.0434\n",
      "  Batch 180/750 - Loss: 0.0214\n",
      "  Batch 190/750 - Loss: 0.1703\n",
      "  Batch 200/750 - Loss: 0.0171\n",
      "  Batch 210/750 - Loss: 0.1284\n",
      "  Batch 220/750 - Loss: 0.1125\n",
      "  Batch 230/750 - Loss: 0.0291\n",
      "  Batch 240/750 - Loss: 0.0308\n",
      "  Batch 250/750 - Loss: 0.0274\n",
      "  Batch 260/750 - Loss: 0.1027\n",
      "  Batch 270/750 - Loss: 0.0427\n",
      "  Batch 280/750 - Loss: 0.0821\n",
      "  Batch 290/750 - Loss: 0.0107\n",
      "  Batch 300/750 - Loss: 0.0268\n",
      "  Batch 310/750 - Loss: 0.0867\n",
      "  Batch 320/750 - Loss: 0.0139\n",
      "  Batch 330/750 - Loss: 0.1173\n",
      "  Batch 340/750 - Loss: 0.1054\n",
      "  Batch 350/750 - Loss: 0.1650\n",
      "  Batch 360/750 - Loss: 0.0331\n",
      "  Batch 370/750 - Loss: 0.0049\n",
      "  Batch 380/750 - Loss: 0.0801\n",
      "  Batch 390/750 - Loss: 0.0399\n",
      "  Batch 400/750 - Loss: 0.2617\n",
      "  Batch 410/750 - Loss: 0.0657\n",
      "  Batch 420/750 - Loss: 0.2398\n",
      "  Batch 430/750 - Loss: 0.0313\n",
      "  Batch 440/750 - Loss: 0.0195\n",
      "  Batch 450/750 - Loss: 0.1279\n",
      "  Batch 460/750 - Loss: 0.0093\n",
      "  Batch 470/750 - Loss: 0.0238\n",
      "  Batch 480/750 - Loss: 0.0394\n",
      "  Batch 490/750 - Loss: 0.0262\n",
      "  Batch 500/750 - Loss: 0.1922\n",
      "  Batch 510/750 - Loss: 0.1593\n",
      "  Batch 520/750 - Loss: 0.0925\n",
      "  Batch 530/750 - Loss: 0.0482\n",
      "  Batch 540/750 - Loss: 0.0325\n",
      "  Batch 550/750 - Loss: 0.0646\n",
      "  Batch 560/750 - Loss: 0.0363\n",
      "  Batch 570/750 - Loss: 0.0455\n",
      "  Batch 580/750 - Loss: 0.0746\n",
      "  Batch 590/750 - Loss: 0.1189\n",
      "  Batch 600/750 - Loss: 0.0085\n",
      "  Batch 610/750 - Loss: 0.0277\n",
      "  Batch 620/750 - Loss: 0.2182\n",
      "  Batch 630/750 - Loss: 0.0365\n",
      "  Batch 640/750 - Loss: 0.1524\n",
      "  Batch 650/750 - Loss: 0.0250\n",
      "  Batch 660/750 - Loss: 0.1151\n",
      "  Batch 670/750 - Loss: 0.0830\n",
      "  Batch 680/750 - Loss: 0.0908\n",
      "  Batch 690/750 - Loss: 0.0385\n",
      "  Batch 700/750 - Loss: 0.0458\n",
      "  Batch 710/750 - Loss: 0.0303\n",
      "  Batch 720/750 - Loss: 0.0033\n",
      "  Batch 730/750 - Loss: 0.1412\n",
      "  Batch 740/750 - Loss: 0.0030\n",
      "  Batch 750/750 - Loss: 0.0147\n",
      "  Epoch [1/10] - Average Loss: 0.0904\n",
      "  Validation Accuracy: 0.9870\n",
      "\n",
      "Epoch [2/10]\n",
      "  Batch 10/750 - Loss: 0.0010\n",
      "  Batch 20/750 - Loss: 0.0047\n",
      "  Batch 30/750 - Loss: 0.0327\n",
      "  Batch 40/750 - Loss: 0.0107\n",
      "  Batch 50/750 - Loss: 0.1151\n",
      "  Batch 60/750 - Loss: 0.0931\n",
      "  Batch 70/750 - Loss: 0.0042\n",
      "  Batch 80/750 - Loss: 0.0767\n",
      "  Batch 90/750 - Loss: 0.0304\n",
      "  Batch 100/750 - Loss: 0.0042\n",
      "  Batch 110/750 - Loss: 0.0286\n",
      "  Batch 120/750 - Loss: 0.1729\n",
      "  Batch 130/750 - Loss: 0.0215\n",
      "  Batch 140/750 - Loss: 0.0041\n",
      "  Batch 150/750 - Loss: 0.1325\n",
      "  Batch 160/750 - Loss: 0.0105\n",
      "  Batch 170/750 - Loss: 0.0110\n",
      "  Batch 180/750 - Loss: 0.0048\n",
      "  Batch 190/750 - Loss: 0.0169\n",
      "  Batch 200/750 - Loss: 0.0006\n",
      "  Batch 210/750 - Loss: 0.0076\n",
      "  Batch 220/750 - Loss: 0.0117\n",
      "  Batch 230/750 - Loss: 0.0504\n",
      "  Batch 240/750 - Loss: 0.0091\n",
      "  Batch 250/750 - Loss: 0.1291\n",
      "  Batch 260/750 - Loss: 0.0179\n",
      "  Batch 270/750 - Loss: 0.0273\n",
      "  Batch 280/750 - Loss: 0.0042\n",
      "  Batch 290/750 - Loss: 0.0421\n",
      "  Batch 300/750 - Loss: 0.0203\n",
      "  Batch 310/750 - Loss: 0.0046\n",
      "  Batch 320/750 - Loss: 0.1788\n",
      "  Batch 330/750 - Loss: 0.0101\n",
      "  Batch 340/750 - Loss: 0.0748\n",
      "  Batch 350/750 - Loss: 0.0431\n",
      "  Batch 360/750 - Loss: 0.0066\n",
      "  Batch 370/750 - Loss: 0.1174\n",
      "  Batch 380/750 - Loss: 0.0171\n",
      "  Batch 390/750 - Loss: 0.0162\n",
      "  Batch 400/750 - Loss: 0.0127\n",
      "  Batch 410/750 - Loss: 0.0426\n",
      "  Batch 420/750 - Loss: 0.0034\n",
      "  Batch 430/750 - Loss: 0.1764\n",
      "  Batch 440/750 - Loss: 0.0277\n",
      "  Batch 450/750 - Loss: 0.1466\n",
      "  Batch 460/750 - Loss: 0.0136\n",
      "  Batch 470/750 - Loss: 0.0135\n",
      "  Batch 480/750 - Loss: 0.0037\n",
      "  Batch 490/750 - Loss: 0.0104\n",
      "  Batch 500/750 - Loss: 0.0866\n",
      "  Batch 510/750 - Loss: 0.1478\n",
      "  Batch 520/750 - Loss: 0.1119\n",
      "  Batch 530/750 - Loss: 0.5058\n",
      "  Batch 540/750 - Loss: 0.2414\n",
      "  Batch 550/750 - Loss: 0.2372\n",
      "  Batch 560/750 - Loss: 0.0337\n",
      "  Batch 570/750 - Loss: 0.0319\n",
      "  Batch 580/750 - Loss: 0.0152\n",
      "  Batch 590/750 - Loss: 0.0584\n",
      "  Batch 600/750 - Loss: 0.1799\n",
      "  Batch 610/750 - Loss: 0.0166\n",
      "  Batch 620/750 - Loss: 0.0032\n",
      "  Batch 630/750 - Loss: 0.0644\n",
      "  Batch 640/750 - Loss: 0.0259\n",
      "  Batch 650/750 - Loss: 0.0181\n",
      "  Batch 660/750 - Loss: 0.0310\n",
      "  Batch 670/750 - Loss: 0.0022\n",
      "  Batch 680/750 - Loss: 0.0126\n",
      "  Batch 690/750 - Loss: 0.0155\n",
      "  Batch 700/750 - Loss: 0.1570\n",
      "  Batch 710/750 - Loss: 0.0137\n",
      "  Batch 720/750 - Loss: 0.1273\n",
      "  Batch 730/750 - Loss: 0.0456\n",
      "  Batch 740/750 - Loss: 0.0352\n",
      "  Batch 750/750 - Loss: 0.0092\n",
      "  Epoch [2/10] - Average Loss: 0.0538\n",
      "  Validation Accuracy: 0.9611\n",
      "\n",
      "Epoch [3/10]\n",
      "  Batch 10/750 - Loss: 0.0073\n",
      "  Batch 20/750 - Loss: 0.0684\n",
      "  Batch 30/750 - Loss: 0.1043\n",
      "  Batch 40/750 - Loss: 0.0618\n",
      "  Batch 50/750 - Loss: 0.0103\n",
      "  Batch 60/750 - Loss: 0.0052\n",
      "  Batch 70/750 - Loss: 0.1661\n",
      "  Batch 80/750 - Loss: 0.0248\n",
      "  Batch 90/750 - Loss: 0.0255\n",
      "  Batch 100/750 - Loss: 0.1444\n",
      "  Batch 110/750 - Loss: 0.0108\n",
      "  Batch 120/750 - Loss: 0.0108\n",
      "  Batch 130/750 - Loss: 0.0016\n",
      "  Batch 140/750 - Loss: 0.0148\n",
      "  Batch 150/750 - Loss: 0.0623\n",
      "  Batch 160/750 - Loss: 0.0049\n",
      "  Batch 170/750 - Loss: 0.0648\n",
      "  Batch 180/750 - Loss: 0.0161\n",
      "  Batch 190/750 - Loss: 0.0136\n",
      "  Batch 200/750 - Loss: 0.0724\n",
      "  Batch 210/750 - Loss: 0.0738\n",
      "  Batch 220/750 - Loss: 0.0462\n",
      "  Batch 230/750 - Loss: 0.1243\n",
      "  Batch 240/750 - Loss: 0.0429\n",
      "  Batch 250/750 - Loss: 0.0011\n",
      "  Batch 260/750 - Loss: 0.0007\n",
      "  Batch 270/750 - Loss: 0.0207\n",
      "  Batch 280/750 - Loss: 0.0174\n",
      "  Batch 290/750 - Loss: 0.0050\n",
      "  Batch 300/750 - Loss: 0.0125\n",
      "  Batch 310/750 - Loss: 0.0059\n",
      "  Batch 320/750 - Loss: 0.0380\n",
      "  Batch 330/750 - Loss: 0.0037\n",
      "  Batch 340/750 - Loss: 0.0520\n",
      "  Batch 350/750 - Loss: 0.0117\n",
      "  Batch 360/750 - Loss: 0.0170\n",
      "  Batch 370/750 - Loss: 0.0005\n",
      "  Batch 380/750 - Loss: 0.0666\n",
      "  Batch 390/750 - Loss: 0.0155\n",
      "  Batch 400/750 - Loss: 0.0030\n",
      "  Batch 410/750 - Loss: 0.0036\n",
      "  Batch 420/750 - Loss: 0.0447\n",
      "  Batch 430/750 - Loss: 0.0397\n",
      "  Batch 440/750 - Loss: 0.1064\n",
      "  Batch 450/750 - Loss: 0.0059\n",
      "  Batch 460/750 - Loss: 0.0488\n",
      "  Batch 470/750 - Loss: 0.0414\n",
      "  Batch 480/750 - Loss: 0.0120\n",
      "  Batch 490/750 - Loss: 0.0015\n",
      "  Batch 500/750 - Loss: 0.0014\n",
      "  Batch 510/750 - Loss: 0.0074\n",
      "  Batch 520/750 - Loss: 0.0188\n",
      "  Batch 530/750 - Loss: 0.0034\n",
      "  Batch 540/750 - Loss: 0.0226\n",
      "  Batch 550/750 - Loss: 0.0058\n",
      "  Batch 560/750 - Loss: 0.0041\n",
      "  Batch 570/750 - Loss: 0.0013\n",
      "  Batch 580/750 - Loss: 0.0260\n",
      "  Batch 590/750 - Loss: 0.0193\n",
      "  Batch 600/750 - Loss: 0.0092\n",
      "  Batch 610/750 - Loss: 0.0662\n",
      "  Batch 620/750 - Loss: 0.1004\n",
      "  Batch 630/750 - Loss: 0.0480\n",
      "  Batch 640/750 - Loss: 0.0086\n",
      "  Batch 650/750 - Loss: 0.0578\n",
      "  Batch 660/750 - Loss: 0.0101\n",
      "  Batch 670/750 - Loss: 0.0018\n",
      "  Batch 680/750 - Loss: 0.0033\n",
      "  Batch 690/750 - Loss: 0.0009\n",
      "  Batch 700/750 - Loss: 0.0131\n",
      "  Batch 710/750 - Loss: 0.0270\n",
      "  Batch 720/750 - Loss: 0.0027\n",
      "  Batch 730/750 - Loss: 0.0282\n",
      "  Batch 740/750 - Loss: 0.0883\n",
      "  Batch 750/750 - Loss: 0.0479\n",
      "  Epoch [3/10] - Average Loss: 0.0399\n",
      "  Validation Accuracy: 0.9908\n",
      "\n",
      "Epoch [4/10]\n",
      "  Batch 10/750 - Loss: 0.0108\n",
      "  Batch 20/750 - Loss: 0.0005\n",
      "  Batch 30/750 - Loss: 0.0975\n",
      "  Batch 40/750 - Loss: 0.0041\n",
      "  Batch 50/750 - Loss: 0.0009\n",
      "  Batch 60/750 - Loss: 0.0676\n",
      "  Batch 70/750 - Loss: 0.1654\n",
      "  Batch 80/750 - Loss: 0.0019\n",
      "  Batch 90/750 - Loss: 0.0063\n",
      "  Batch 100/750 - Loss: 0.0931\n",
      "  Batch 110/750 - Loss: 0.0024\n",
      "  Batch 120/750 - Loss: 0.0018\n",
      "  Batch 130/750 - Loss: 0.1193\n",
      "  Batch 140/750 - Loss: 0.0100\n",
      "  Batch 150/750 - Loss: 0.0237\n",
      "  Batch 160/750 - Loss: 0.0046\n",
      "  Batch 170/750 - Loss: 0.0080\n",
      "  Batch 180/750 - Loss: 0.0042\n",
      "  Batch 190/750 - Loss: 0.0112\n",
      "  Batch 200/750 - Loss: 0.0005\n",
      "  Batch 210/750 - Loss: 0.0304\n",
      "  Batch 220/750 - Loss: 0.0410\n",
      "  Batch 230/750 - Loss: 0.0324\n",
      "  Batch 240/750 - Loss: 0.0063\n",
      "  Batch 250/750 - Loss: 0.0634\n",
      "  Batch 260/750 - Loss: 0.0157\n",
      "  Batch 270/750 - Loss: 0.0035\n",
      "  Batch 280/750 - Loss: 0.0016\n",
      "  Batch 290/750 - Loss: 0.0005\n",
      "  Batch 300/750 - Loss: 0.0025\n",
      "  Batch 310/750 - Loss: 0.0709\n",
      "  Batch 320/750 - Loss: 0.0110\n",
      "  Batch 330/750 - Loss: 0.0019\n",
      "  Batch 340/750 - Loss: 0.0155\n",
      "  Batch 350/750 - Loss: 0.0178\n",
      "  Batch 360/750 - Loss: 0.0580\n",
      "  Batch 370/750 - Loss: 0.0546\n",
      "  Batch 380/750 - Loss: 0.0735\n",
      "  Batch 390/750 - Loss: 0.1157\n",
      "  Batch 400/750 - Loss: 0.0013\n",
      "  Batch 410/750 - Loss: 0.0017\n",
      "  Batch 420/750 - Loss: 0.0349\n",
      "  Batch 430/750 - Loss: 0.0371\n",
      "  Batch 440/750 - Loss: 0.0470\n",
      "  Batch 450/750 - Loss: 0.1152\n",
      "  Batch 460/750 - Loss: 0.0544\n",
      "  Batch 470/750 - Loss: 0.0013\n",
      "  Batch 480/750 - Loss: 0.0252\n",
      "  Batch 490/750 - Loss: 0.0975\n",
      "  Batch 500/750 - Loss: 0.0155\n",
      "  Batch 510/750 - Loss: 0.0288\n",
      "  Batch 520/750 - Loss: 0.0900\n",
      "  Batch 530/750 - Loss: 0.0052\n",
      "  Batch 540/750 - Loss: 0.1593\n",
      "  Batch 550/750 - Loss: 0.0404\n",
      "  Batch 560/750 - Loss: 0.0247\n",
      "  Batch 570/750 - Loss: 0.0128\n",
      "  Batch 580/750 - Loss: 0.0786\n",
      "  Batch 590/750 - Loss: 0.0018\n",
      "  Batch 600/750 - Loss: 0.0005\n",
      "  Batch 610/750 - Loss: 0.1390\n",
      "  Batch 620/750 - Loss: 0.0008\n",
      "  Batch 630/750 - Loss: 0.0088\n",
      "  Batch 640/750 - Loss: 0.0006\n",
      "  Batch 650/750 - Loss: 0.1639\n",
      "  Batch 660/750 - Loss: 0.0020\n",
      "  Batch 670/750 - Loss: 0.0376\n",
      "  Batch 680/750 - Loss: 0.0317\n",
      "  Batch 690/750 - Loss: 0.0829\n",
      "  Batch 700/750 - Loss: 0.1319\n",
      "  Batch 710/750 - Loss: 0.0256\n",
      "  Batch 720/750 - Loss: 0.0844\n",
      "  Batch 730/750 - Loss: 0.1140\n",
      "  Batch 740/750 - Loss: 0.0738\n",
      "  Batch 750/750 - Loss: 0.0188\n",
      "  Epoch [4/10] - Average Loss: 0.0309\n",
      "  Validation Accuracy: 0.9924\n",
      "\n",
      "Epoch [5/10]\n",
      "  Batch 10/750 - Loss: 0.0802\n",
      "  Batch 20/750 - Loss: 0.0014\n",
      "  Batch 30/750 - Loss: 0.0575\n",
      "  Batch 40/750 - Loss: 0.0061\n",
      "  Batch 50/750 - Loss: 0.0028\n",
      "  Batch 60/750 - Loss: 0.0454\n",
      "  Batch 70/750 - Loss: 0.0700\n",
      "  Batch 80/750 - Loss: 0.0232\n",
      "  Batch 90/750 - Loss: 0.0454\n",
      "  Batch 100/750 - Loss: 0.0477\n",
      "  Batch 110/750 - Loss: 0.0012\n",
      "  Batch 120/750 - Loss: 0.0823\n",
      "  Batch 130/750 - Loss: 0.0165\n",
      "  Batch 140/750 - Loss: 0.0331\n",
      "  Batch 150/750 - Loss: 0.0009\n",
      "  Batch 160/750 - Loss: 0.0542\n",
      "  Batch 170/750 - Loss: 0.0323\n",
      "  Batch 180/750 - Loss: 0.1841\n",
      "  Batch 190/750 - Loss: 0.0173\n",
      "  Batch 200/750 - Loss: 0.0272\n",
      "  Batch 210/750 - Loss: 0.0557\n",
      "  Batch 220/750 - Loss: 0.0035\n",
      "  Batch 230/750 - Loss: 0.0072\n",
      "  Batch 240/750 - Loss: 0.0027\n",
      "  Batch 250/750 - Loss: 0.1090\n",
      "  Batch 260/750 - Loss: 0.0006\n",
      "  Batch 270/750 - Loss: 0.0495\n",
      "  Batch 280/750 - Loss: 0.0183\n",
      "  Batch 290/750 - Loss: 0.0265\n",
      "  Batch 300/750 - Loss: 0.0082\n",
      "  Batch 310/750 - Loss: 0.0247\n",
      "  Batch 320/750 - Loss: 0.0018\n",
      "  Batch 330/750 - Loss: 0.0826\n",
      "  Batch 340/750 - Loss: 0.1504\n",
      "  Batch 350/750 - Loss: 0.0018\n",
      "  Batch 360/750 - Loss: 0.0289\n",
      "  Batch 370/750 - Loss: 0.0016\n",
      "  Batch 380/750 - Loss: 0.0432\n",
      "  Batch 390/750 - Loss: 0.0054\n",
      "  Batch 400/750 - Loss: 0.0281\n",
      "  Batch 410/750 - Loss: 0.0019\n",
      "  Batch 420/750 - Loss: 0.0035\n",
      "  Batch 430/750 - Loss: 0.0193\n",
      "  Batch 440/750 - Loss: 0.0011\n",
      "  Batch 450/750 - Loss: 0.1289\n",
      "  Batch 460/750 - Loss: 0.0668\n",
      "  Batch 470/750 - Loss: 0.0178\n",
      "  Batch 480/750 - Loss: 0.0178\n",
      "  Batch 490/750 - Loss: 0.0025\n",
      "  Batch 500/750 - Loss: 0.0015\n",
      "  Batch 510/750 - Loss: 0.0036\n",
      "  Batch 520/750 - Loss: 0.0139\n",
      "  Batch 530/750 - Loss: 0.0095\n",
      "  Batch 540/750 - Loss: 0.0003\n",
      "  Batch 550/750 - Loss: 0.0665\n",
      "  Batch 560/750 - Loss: 0.0039\n",
      "  Batch 570/750 - Loss: 0.0976\n",
      "  Batch 580/750 - Loss: 0.0924\n",
      "  Batch 590/750 - Loss: 0.0127\n",
      "  Batch 600/750 - Loss: 0.0568\n",
      "  Batch 610/750 - Loss: 0.0544\n",
      "  Batch 620/750 - Loss: 0.0200\n",
      "  Batch 630/750 - Loss: 0.0086\n",
      "  Batch 640/750 - Loss: 0.0293\n",
      "  Batch 650/750 - Loss: 0.0497\n",
      "  Batch 660/750 - Loss: 0.1127\n",
      "  Batch 670/750 - Loss: 0.0188\n",
      "  Batch 680/750 - Loss: 0.0317\n",
      "  Batch 690/750 - Loss: 0.0047\n",
      "  Batch 700/750 - Loss: 0.0028\n",
      "  Batch 710/750 - Loss: 0.0181\n",
      "  Batch 720/750 - Loss: 0.0308\n",
      "  Batch 730/750 - Loss: 0.0045\n",
      "  Batch 740/750 - Loss: 0.0592\n",
      "  Batch 750/750 - Loss: 0.0056\n",
      "  Epoch [5/10] - Average Loss: 0.0302\n",
      "  Validation Accuracy: 0.9851\n",
      "\n",
      "Epoch [6/10]\n",
      "  Batch 10/750 - Loss: 0.0227\n",
      "  Batch 20/750 - Loss: 0.0014\n",
      "  Batch 30/750 - Loss: 0.0965\n",
      "  Batch 40/750 - Loss: 0.0007\n",
      "  Batch 50/750 - Loss: 0.0353\n",
      "  Batch 60/750 - Loss: 0.0065\n",
      "  Batch 70/750 - Loss: 0.0031\n",
      "  Batch 80/750 - Loss: 0.0557\n",
      "  Batch 90/750 - Loss: 0.0791\n",
      "  Batch 100/750 - Loss: 0.0225\n",
      "  Batch 110/750 - Loss: 0.0069\n",
      "  Batch 120/750 - Loss: 0.0171\n",
      "  Batch 130/750 - Loss: 0.1382\n",
      "  Batch 140/750 - Loss: 0.0099\n",
      "  Batch 150/750 - Loss: 0.0561\n",
      "  Batch 160/750 - Loss: 0.0066\n",
      "  Batch 170/750 - Loss: 0.0042\n",
      "  Batch 180/750 - Loss: 0.0348\n",
      "  Batch 190/750 - Loss: 0.0165\n",
      "  Batch 200/750 - Loss: 0.0035\n",
      "  Batch 210/750 - Loss: 0.0623\n",
      "  Batch 220/750 - Loss: 0.0380\n",
      "  Batch 230/750 - Loss: 0.0018\n",
      "  Batch 240/750 - Loss: 0.0187\n",
      "  Batch 250/750 - Loss: 0.0108\n",
      "  Batch 260/750 - Loss: 0.0010\n",
      "  Batch 270/750 - Loss: 0.0253\n",
      "  Batch 280/750 - Loss: 0.0058\n",
      "  Batch 290/750 - Loss: 0.0033\n",
      "  Batch 300/750 - Loss: 0.0163\n",
      "  Batch 310/750 - Loss: 0.0075\n",
      "  Batch 320/750 - Loss: 0.0446\n",
      "  Batch 330/750 - Loss: 0.0061\n",
      "  Batch 340/750 - Loss: 0.0306\n",
      "  Batch 350/750 - Loss: 0.0222\n",
      "  Batch 360/750 - Loss: 0.0391\n",
      "  Batch 370/750 - Loss: 0.0003\n",
      "  Batch 380/750 - Loss: 0.0507\n",
      "  Batch 390/750 - Loss: 0.0100\n",
      "  Batch 400/750 - Loss: 0.0107\n",
      "  Batch 410/750 - Loss: 0.0323\n",
      "  Batch 420/750 - Loss: 0.0225\n",
      "  Batch 430/750 - Loss: 0.0025\n",
      "  Batch 440/750 - Loss: 0.0023\n",
      "  Batch 450/750 - Loss: 0.0121\n",
      "  Batch 460/750 - Loss: 0.0084\n",
      "  Batch 470/750 - Loss: 0.0373\n",
      "  Batch 480/750 - Loss: 0.0600\n",
      "  Batch 490/750 - Loss: 0.0360\n",
      "  Batch 500/750 - Loss: 0.0031\n",
      "  Batch 510/750 - Loss: 0.0048\n",
      "  Batch 520/750 - Loss: 0.0029\n",
      "  Batch 530/750 - Loss: 0.0029\n",
      "  Batch 540/750 - Loss: 0.0030\n",
      "  Batch 550/750 - Loss: 0.0016\n",
      "  Batch 560/750 - Loss: 0.0145\n",
      "  Batch 570/750 - Loss: 0.0615\n",
      "  Batch 580/750 - Loss: 0.0747\n",
      "  Batch 590/750 - Loss: 0.0028\n",
      "  Batch 600/750 - Loss: 0.0423\n",
      "  Batch 610/750 - Loss: 0.0140\n",
      "  Batch 620/750 - Loss: 0.0875\n",
      "  Batch 630/750 - Loss: 0.0231\n",
      "  Batch 640/750 - Loss: 0.0352\n",
      "  Batch 650/750 - Loss: 0.0244\n",
      "  Batch 660/750 - Loss: 0.0760\n",
      "  Batch 670/750 - Loss: 0.0479\n",
      "  Batch 680/750 - Loss: 0.0575\n",
      "  Batch 690/750 - Loss: 0.0016\n",
      "  Batch 700/750 - Loss: 0.0198\n",
      "  Batch 710/750 - Loss: 0.0502\n",
      "  Batch 720/750 - Loss: 0.0016\n",
      "  Batch 730/750 - Loss: 0.0106\n",
      "  Batch 740/750 - Loss: 0.0030\n",
      "  Batch 750/750 - Loss: 0.0102\n",
      "  Epoch [6/10] - Average Loss: 0.0248\n",
      "  Validation Accuracy: 0.9917\n",
      "\n",
      "Epoch [7/10]\n",
      "  Batch 10/750 - Loss: 0.0059\n",
      "  Batch 20/750 - Loss: 0.0003\n",
      "  Batch 30/750 - Loss: 0.0084\n",
      "  Batch 40/750 - Loss: 0.0236\n",
      "  Batch 50/750 - Loss: 0.0008\n",
      "  Batch 60/750 - Loss: 0.0022\n",
      "  Batch 70/750 - Loss: 0.0020\n",
      "  Batch 80/750 - Loss: 0.0014\n",
      "  Batch 90/750 - Loss: 0.0197\n",
      "  Batch 100/750 - Loss: 0.0200\n",
      "  Batch 110/750 - Loss: 0.0878\n",
      "  Batch 120/750 - Loss: 0.0018\n",
      "  Batch 130/750 - Loss: 0.0018\n",
      "  Batch 140/750 - Loss: 0.0045\n",
      "  Batch 150/750 - Loss: 0.0285\n",
      "  Batch 160/750 - Loss: 0.0027\n",
      "  Batch 170/750 - Loss: 0.0369\n",
      "  Batch 180/750 - Loss: 0.0199\n",
      "  Batch 190/750 - Loss: 0.0043\n",
      "  Batch 200/750 - Loss: 0.0173\n",
      "  Batch 210/750 - Loss: 0.0056\n",
      "  Batch 220/750 - Loss: 0.0116\n",
      "  Batch 230/750 - Loss: 0.0093\n",
      "  Batch 240/750 - Loss: 0.0017\n",
      "  Batch 250/750 - Loss: 0.0393\n",
      "  Batch 260/750 - Loss: 0.0037\n",
      "  Batch 270/750 - Loss: 0.0063\n",
      "  Batch 280/750 - Loss: 0.0005\n",
      "  Batch 290/750 - Loss: 0.0162\n",
      "  Batch 300/750 - Loss: 0.0005\n",
      "  Batch 310/750 - Loss: 0.0005\n",
      "  Batch 320/750 - Loss: 0.0001\n",
      "  Batch 330/750 - Loss: 0.0001\n",
      "  Batch 340/750 - Loss: 0.1118\n",
      "  Batch 350/750 - Loss: 0.0015\n",
      "  Batch 360/750 - Loss: 0.0004\n",
      "  Batch 370/750 - Loss: 0.0418\n",
      "  Batch 380/750 - Loss: 0.0011\n",
      "  Batch 390/750 - Loss: 0.1100\n",
      "  Batch 400/750 - Loss: 0.0096\n",
      "  Batch 410/750 - Loss: 0.0006\n",
      "  Batch 420/750 - Loss: 0.0403\n",
      "  Batch 430/750 - Loss: 0.0018\n",
      "  Batch 440/750 - Loss: 0.0007\n",
      "  Batch 450/750 - Loss: 0.0543\n",
      "  Batch 460/750 - Loss: 0.0107\n",
      "  Batch 470/750 - Loss: 0.0036\n",
      "  Batch 480/750 - Loss: 0.0846\n",
      "  Batch 490/750 - Loss: 0.0274\n",
      "  Batch 500/750 - Loss: 0.0302\n",
      "  Batch 510/750 - Loss: 0.0091\n",
      "  Batch 520/750 - Loss: 0.0157\n",
      "  Batch 530/750 - Loss: 0.0671\n",
      "  Batch 540/750 - Loss: 0.0007\n",
      "  Batch 550/750 - Loss: 0.0029\n",
      "  Batch 560/750 - Loss: 0.0077\n",
      "  Batch 570/750 - Loss: 0.0004\n",
      "  Batch 580/750 - Loss: 0.0005\n",
      "  Batch 590/750 - Loss: 0.1847\n",
      "  Batch 600/750 - Loss: 0.0716\n",
      "  Batch 610/750 - Loss: 0.0163\n",
      "  Batch 620/750 - Loss: 0.0465\n",
      "  Batch 630/750 - Loss: 0.0166\n",
      "  Batch 640/750 - Loss: 0.0063\n",
      "  Batch 650/750 - Loss: 0.0003\n",
      "  Batch 660/750 - Loss: 0.0031\n",
      "  Batch 670/750 - Loss: 0.0113\n",
      "  Batch 680/750 - Loss: 0.0037\n",
      "  Batch 690/750 - Loss: 0.0047\n",
      "  Batch 700/750 - Loss: 0.0085\n",
      "  Batch 710/750 - Loss: 0.0089\n",
      "  Batch 720/750 - Loss: 0.0340\n",
      "  Batch 730/750 - Loss: 0.0004\n",
      "  Batch 740/750 - Loss: 0.0114\n",
      "  Batch 750/750 - Loss: 0.0164\n",
      "  Epoch [7/10] - Average Loss: 0.0259\n",
      "  Validation Accuracy: 0.9915\n",
      "\n",
      "Epoch [8/10]\n",
      "  Batch 10/750 - Loss: 0.0087\n",
      "  Batch 20/750 - Loss: 0.0000\n",
      "  Batch 30/750 - Loss: 0.0023\n",
      "  Batch 40/750 - Loss: 0.0539\n",
      "  Batch 50/750 - Loss: 0.0219\n",
      "  Batch 60/750 - Loss: 0.0043\n",
      "  Batch 70/750 - Loss: 0.0012\n",
      "  Batch 80/750 - Loss: 0.0191\n",
      "  Batch 90/750 - Loss: 0.0131\n",
      "  Batch 100/750 - Loss: 0.0027\n",
      "  Batch 110/750 - Loss: 0.1111\n",
      "  Batch 120/750 - Loss: 0.1234\n",
      "  Batch 130/750 - Loss: 0.0029\n",
      "  Batch 140/750 - Loss: 0.0012\n",
      "  Batch 150/750 - Loss: 0.0486\n",
      "  Batch 160/750 - Loss: 0.0008\n",
      "  Batch 170/750 - Loss: 0.0004\n",
      "  Batch 180/750 - Loss: 0.0006\n",
      "  Batch 190/750 - Loss: 0.0011\n",
      "  Batch 200/750 - Loss: 0.0015\n",
      "  Batch 210/750 - Loss: 0.0264\n",
      "  Batch 220/750 - Loss: 0.0129\n",
      "  Batch 230/750 - Loss: 0.0023\n",
      "  Batch 240/750 - Loss: 0.0020\n",
      "  Batch 250/750 - Loss: 0.0006\n",
      "  Batch 260/750 - Loss: 0.0254\n",
      "  Batch 270/750 - Loss: 0.0129\n",
      "  Batch 280/750 - Loss: 0.0075\n",
      "  Batch 290/750 - Loss: 0.0028\n",
      "  Batch 300/750 - Loss: 0.0313\n",
      "  Batch 310/750 - Loss: 0.0107\n",
      "  Batch 320/750 - Loss: 0.0003\n",
      "  Batch 330/750 - Loss: 0.0651\n",
      "  Batch 340/750 - Loss: 0.0508\n",
      "  Batch 350/750 - Loss: 0.0050\n",
      "  Batch 360/750 - Loss: 0.0773\n",
      "  Batch 370/750 - Loss: 0.0101\n",
      "  Batch 380/750 - Loss: 0.0080\n",
      "  Batch 390/750 - Loss: 0.0039\n",
      "  Batch 400/750 - Loss: 0.0415\n",
      "  Batch 410/750 - Loss: 0.0018\n",
      "  Batch 420/750 - Loss: 0.0109\n",
      "  Batch 430/750 - Loss: 0.0411\n",
      "  Batch 440/750 - Loss: 0.0510\n",
      "  Batch 450/750 - Loss: 0.0014\n",
      "  Batch 460/750 - Loss: 0.0026\n",
      "  Batch 470/750 - Loss: 0.0006\n",
      "  Batch 480/750 - Loss: 0.0255\n",
      "  Batch 490/750 - Loss: 0.0013\n",
      "  Batch 500/750 - Loss: 0.0047\n",
      "  Batch 510/750 - Loss: 0.0459\n",
      "  Batch 520/750 - Loss: 0.0018\n",
      "  Batch 530/750 - Loss: 0.0083\n",
      "  Batch 540/750 - Loss: 0.1951\n",
      "  Batch 550/750 - Loss: 0.0005\n",
      "  Batch 560/750 - Loss: 0.0014\n",
      "  Batch 570/750 - Loss: 0.0011\n",
      "  Batch 580/750 - Loss: 0.0002\n",
      "  Batch 590/750 - Loss: 0.0483\n",
      "  Batch 600/750 - Loss: 0.0298\n",
      "  Batch 610/750 - Loss: 0.0587\n",
      "  Batch 620/750 - Loss: 0.0010\n",
      "  Batch 630/750 - Loss: 0.0234\n",
      "  Batch 640/750 - Loss: 0.0039\n",
      "  Batch 650/750 - Loss: 0.0007\n",
      "  Batch 660/750 - Loss: 0.0009\n",
      "  Batch 670/750 - Loss: 0.0682\n",
      "  Batch 680/750 - Loss: 0.0756\n",
      "  Batch 690/750 - Loss: 0.1037\n",
      "  Batch 700/750 - Loss: 0.0131\n",
      "  Batch 710/750 - Loss: 0.0082\n",
      "  Batch 720/750 - Loss: 0.1017\n",
      "  Batch 730/750 - Loss: 0.0005\n",
      "  Batch 740/750 - Loss: 0.0541\n",
      "  Batch 750/750 - Loss: 0.0029\n",
      "  Epoch [8/10] - Average Loss: 0.0238\n",
      "  Validation Accuracy: 0.9890\n",
      "\n",
      "Epoch [9/10]\n",
      "  Batch 10/750 - Loss: 0.0493\n",
      "  Batch 20/750 - Loss: 0.0027\n",
      "  Batch 30/750 - Loss: 0.0188\n",
      "  Batch 40/750 - Loss: 0.0212\n",
      "  Batch 50/750 - Loss: 0.0030\n",
      "  Batch 60/750 - Loss: 0.0278\n",
      "  Batch 70/750 - Loss: 0.0005\n",
      "  Batch 80/750 - Loss: 0.0005\n",
      "  Batch 90/750 - Loss: 0.0000\n",
      "  Batch 100/750 - Loss: 0.0188\n",
      "  Batch 110/750 - Loss: 0.0125\n",
      "  Batch 120/750 - Loss: 0.0033\n",
      "  Batch 130/750 - Loss: 0.0016\n",
      "  Batch 140/750 - Loss: 0.0049\n",
      "  Batch 150/750 - Loss: 0.0034\n",
      "  Batch 160/750 - Loss: 0.0315\n",
      "  Batch 170/750 - Loss: 0.0169\n",
      "  Batch 180/750 - Loss: 0.0047\n",
      "  Batch 190/750 - Loss: 0.0241\n",
      "  Batch 200/750 - Loss: 0.0030\n",
      "  Batch 210/750 - Loss: 0.0318\n",
      "  Batch 220/750 - Loss: 0.0002\n",
      "  Batch 230/750 - Loss: 0.0004\n",
      "  Batch 240/750 - Loss: 0.0121\n",
      "  Batch 250/750 - Loss: 0.0036\n",
      "  Batch 260/750 - Loss: 0.0149\n",
      "  Batch 270/750 - Loss: 0.0042\n",
      "  Batch 280/750 - Loss: 0.0013\n",
      "  Batch 290/750 - Loss: 0.0006\n",
      "  Batch 300/750 - Loss: 0.0596\n",
      "  Batch 310/750 - Loss: 0.0038\n",
      "  Batch 320/750 - Loss: 0.0054\n",
      "  Batch 330/750 - Loss: 0.0002\n",
      "  Batch 340/750 - Loss: 0.0117\n",
      "  Batch 350/750 - Loss: 0.0029\n",
      "  Batch 360/750 - Loss: 0.0638\n",
      "  Batch 370/750 - Loss: 0.0007\n",
      "  Batch 380/750 - Loss: 0.0037\n",
      "  Batch 390/750 - Loss: 0.0000\n",
      "  Batch 400/750 - Loss: 0.0002\n",
      "  Batch 410/750 - Loss: 0.0068\n",
      "  Batch 420/750 - Loss: 0.0128\n",
      "  Batch 430/750 - Loss: 0.0209\n",
      "  Batch 440/750 - Loss: 0.0007\n",
      "  Batch 450/750 - Loss: 0.0013\n",
      "  Batch 460/750 - Loss: 0.0387\n",
      "  Batch 470/750 - Loss: 0.0828\n",
      "  Batch 480/750 - Loss: 0.0541\n",
      "  Batch 490/750 - Loss: 0.0708\n",
      "  Batch 500/750 - Loss: 0.0046\n",
      "  Batch 510/750 - Loss: 0.0184\n",
      "  Batch 520/750 - Loss: 0.0132\n",
      "  Batch 530/750 - Loss: 0.0871\n",
      "  Batch 540/750 - Loss: 0.0817\n",
      "  Batch 550/750 - Loss: 0.0368\n",
      "  Batch 560/750 - Loss: 0.1888\n",
      "  Batch 570/750 - Loss: 0.0288\n",
      "  Batch 580/750 - Loss: 0.0271\n",
      "  Batch 590/750 - Loss: 0.0141\n",
      "  Batch 600/750 - Loss: 0.0123\n",
      "  Batch 610/750 - Loss: 0.0492\n",
      "  Batch 620/750 - Loss: 0.0099\n",
      "  Batch 630/750 - Loss: 0.0008\n",
      "  Batch 640/750 - Loss: 0.0464\n",
      "  Batch 650/750 - Loss: 0.0294\n",
      "  Batch 660/750 - Loss: 0.0333\n",
      "  Batch 670/750 - Loss: 0.0025\n",
      "  Batch 680/750 - Loss: 0.0429\n",
      "  Batch 690/750 - Loss: 0.0004\n",
      "  Batch 700/750 - Loss: 0.0559\n",
      "  Batch 710/750 - Loss: 0.0197\n",
      "  Batch 720/750 - Loss: 0.0227\n",
      "  Batch 730/750 - Loss: 0.0006\n",
      "  Batch 740/750 - Loss: 0.0477\n",
      "  Batch 750/750 - Loss: 0.0024\n",
      "  Epoch [9/10] - Average Loss: 0.0258\n",
      "  Validation Accuracy: 0.9852\n",
      "\n",
      "Epoch [10/10]\n",
      "  Batch 10/750 - Loss: 0.0025\n",
      "  Batch 20/750 - Loss: 0.0170\n",
      "  Batch 30/750 - Loss: 0.0005\n",
      "  Batch 40/750 - Loss: 0.0677\n",
      "  Batch 50/750 - Loss: 0.0001\n",
      "  Batch 60/750 - Loss: 0.0225\n",
      "  Batch 70/750 - Loss: 0.0015\n",
      "  Batch 80/750 - Loss: 0.1261\n",
      "  Batch 90/750 - Loss: 0.0096\n",
      "  Batch 100/750 - Loss: 0.0338\n",
      "  Batch 110/750 - Loss: 0.0006\n",
      "  Batch 120/750 - Loss: 0.0009\n",
      "  Batch 130/750 - Loss: 0.0013\n",
      "  Batch 140/750 - Loss: 0.0597\n",
      "  Batch 150/750 - Loss: 0.0003\n",
      "  Batch 160/750 - Loss: 0.0157\n",
      "  Batch 170/750 - Loss: 0.0070\n",
      "  Batch 180/750 - Loss: 0.0063\n",
      "  Batch 190/750 - Loss: 0.0420\n",
      "  Batch 200/750 - Loss: 0.0160\n",
      "  Batch 210/750 - Loss: 0.0052\n",
      "  Batch 220/750 - Loss: 0.0062\n",
      "  Batch 230/750 - Loss: 0.0017\n",
      "  Batch 240/750 - Loss: 0.0012\n",
      "  Batch 250/750 - Loss: 0.0121\n",
      "  Batch 260/750 - Loss: 0.0886\n",
      "  Batch 270/750 - Loss: 0.0192\n",
      "  Batch 280/750 - Loss: 0.0060\n",
      "  Batch 290/750 - Loss: 0.0005\n",
      "  Batch 300/750 - Loss: 0.0832\n",
      "  Batch 310/750 - Loss: 0.0477\n",
      "  Batch 320/750 - Loss: 0.0023\n",
      "  Batch 330/750 - Loss: 0.0112\n",
      "  Batch 340/750 - Loss: 0.0019\n",
      "  Batch 350/750 - Loss: 0.0049\n",
      "  Batch 360/750 - Loss: 0.0279\n",
      "  Batch 370/750 - Loss: 0.0002\n",
      "  Batch 380/750 - Loss: 0.0140\n",
      "  Batch 390/750 - Loss: 0.0557\n",
      "  Batch 400/750 - Loss: 0.0006\n",
      "  Batch 410/750 - Loss: 0.0023\n",
      "  Batch 420/750 - Loss: 0.0786\n",
      "  Batch 430/750 - Loss: 0.0022\n",
      "  Batch 440/750 - Loss: 0.0336\n",
      "  Batch 450/750 - Loss: 0.0109\n",
      "  Batch 460/750 - Loss: 0.0006\n",
      "  Batch 470/750 - Loss: 0.0009\n",
      "  Batch 480/750 - Loss: 0.0105\n",
      "  Batch 490/750 - Loss: 0.0015\n",
      "  Batch 500/750 - Loss: 0.0012\n",
      "  Batch 510/750 - Loss: 0.0044\n",
      "  Batch 520/750 - Loss: 0.0052\n",
      "  Batch 530/750 - Loss: 0.0006\n",
      "  Batch 540/750 - Loss: 0.0001\n",
      "  Batch 550/750 - Loss: 0.0002\n",
      "  Batch 560/750 - Loss: 0.0090\n",
      "  Batch 570/750 - Loss: 0.0129\n",
      "  Batch 580/750 - Loss: 0.0012\n",
      "  Batch 590/750 - Loss: 0.0138\n",
      "  Batch 600/750 - Loss: 0.0001\n",
      "  Batch 610/750 - Loss: 0.0033\n",
      "  Batch 620/750 - Loss: 0.0151\n",
      "  Batch 630/750 - Loss: 0.0032\n",
      "  Batch 640/750 - Loss: 0.1562\n",
      "  Batch 650/750 - Loss: 0.0001\n",
      "  Batch 660/750 - Loss: 0.0130\n",
      "  Batch 670/750 - Loss: 0.0026\n",
      "  Batch 680/750 - Loss: 0.0140\n",
      "  Batch 690/750 - Loss: 0.0031\n",
      "  Batch 700/750 - Loss: 0.0349\n",
      "  Batch 710/750 - Loss: 0.0144\n",
      "  Batch 720/750 - Loss: 0.0025\n",
      "  Batch 730/750 - Loss: 0.0004\n",
      "  Batch 740/750 - Loss: 0.0004\n",
      "  Batch 750/750 - Loss: 0.0009\n",
      "  Epoch [10/10] - Average Loss: 0.0207\n",
      "  Validation Accuracy: 0.9925\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Ensure dataset loading and model instantiation are done before this code\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Move the existing model to the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (datasets should be loaded before this)\n",
    "torch.manual_seed(44)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")  # Print epoch number\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(training_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print progress for every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"  Batch {batch_idx+1}/{len(training_dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(training_dataloader)  # Correct loss averaging\n",
    "    print(f\"  Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"  Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nTraining Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. ### EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 1.0000\n",
      "Test Accuracy : 0.9922\n",
      "Test Accuracy : 0.9948\n",
      "Test Accuracy : 0.9922\n",
      "Test Accuracy : 0.9938\n",
      "Test Accuracy : 0.9896\n",
      "Test Accuracy : 0.9866\n",
      "Test Accuracy : 0.9883\n",
      "Test Accuracy : 0.9878\n",
      "Test Accuracy : 0.9875\n",
      "Test Accuracy : 0.9872\n",
      "Test Accuracy : 0.9883\n",
      "Test Accuracy : 0.9892\n",
      "Test Accuracy : 0.9900\n",
      "Test Accuracy : 0.9875\n",
      "Test Accuracy : 0.9873\n",
      "Test Accuracy : 0.9871\n",
      "Test Accuracy : 0.9870\n",
      "Test Accuracy : 0.9877\n",
      "Test Accuracy : 0.9859\n",
      "Test Accuracy : 0.9866\n",
      "Test Accuracy : 0.9865\n",
      "Test Accuracy : 0.9871\n",
      "Test Accuracy : 0.9876\n",
      "Test Accuracy : 0.9881\n",
      "Test Accuracy : 0.9886\n",
      "Test Accuracy : 0.9890\n",
      "Test Accuracy : 0.9883\n",
      "Test Accuracy : 0.9887\n",
      "Test Accuracy : 0.9880\n",
      "Test Accuracy : 0.9884\n",
      "Test Accuracy : 0.9883\n",
      "Test Accuracy : 0.9882\n",
      "Test Accuracy : 0.9876\n",
      "Test Accuracy : 0.9879\n",
      "Test Accuracy : 0.9883\n",
      "Test Accuracy : 0.9882\n",
      "Test Accuracy : 0.9885\n",
      "Test Accuracy : 0.9880\n",
      "Test Accuracy : 0.9879\n",
      "Test Accuracy : 0.9878\n",
      "Test Accuracy : 0.9877\n",
      "Test Accuracy : 0.9880\n",
      "Test Accuracy : 0.9879\n",
      "Test Accuracy : 0.9882\n",
      "Test Accuracy : 0.9881\n",
      "Test Accuracy : 0.9884\n",
      "Test Accuracy : 0.9880\n",
      "Test Accuracy : 0.9882\n",
      "Test Accuracy : 0.9884\n",
      "Test Accuracy : 0.9887\n",
      "Test Accuracy : 0.9889\n",
      "Test Accuracy : 0.9888\n",
      "Test Accuracy : 0.9887\n",
      "Test Accuracy : 0.9889\n",
      "Test Accuracy : 0.9886\n",
      "Test Accuracy : 0.9888\n",
      "Test Accuracy : 0.9890\n",
      "Test Accuracy : 0.9889\n",
      "Test Accuracy : 0.9891\n",
      "Test Accuracy : 0.9892\n",
      "Test Accuracy : 0.9892\n",
      "Test Accuracy : 0.9893\n",
      "Test Accuracy : 0.9895\n",
      "Test Accuracy : 0.9897\n",
      "Test Accuracy : 0.9898\n",
      "Test Accuracy : 0.9900\n",
      "Test Accuracy : 0.9901\n",
      "Test Accuracy : 0.9900\n",
      "Test Accuracy : 0.9902\n",
      "Test Accuracy : 0.9899\n",
      "Test Accuracy : 0.9900\n",
      "Test Accuracy : 0.9902\n",
      "Test Accuracy : 0.9901\n",
      "Test Accuracy : 0.9900\n",
      "Test Accuracy : 0.9897\n",
      "Test Accuracy : 0.9894\n",
      "Test Accuracy : 0.9896\n",
      "Test Accuracy : 0.9897\n",
      "Test Accuracy : 0.9898\n",
      "Test Accuracy : 0.9898\n",
      "Test Accuracy : 0.9899\n",
      "Test Accuracy : 0.9900\n",
      "Test Accuracy : 0.9901\n",
      "Test Accuracy : 0.9903\n",
      "Test Accuracy : 0.9902\n",
      "Test Accuracy : 0.9903\n",
      "Test Accuracy : 0.9904\n",
      "Test Accuracy : 0.9905\n",
      "Test Accuracy : 0.9906\n",
      "Test Accuracy : 0.9907\n",
      "Test Accuracy : 0.9908\n",
      "Test Accuracy : 0.9908\n",
      "Test Accuracy : 0.9905\n",
      "Test Accuracy : 0.9906\n",
      "Test Accuracy : 0.9907\n",
      "Test Accuracy : 0.9907\n",
      "Test Accuracy : 0.9908\n",
      "Test Accuracy : 0.9908\n",
      "Test Accuracy : 0.9909\n",
      "Test Accuracy : 0.9910\n",
      "Test Accuracy : 0.9911\n",
      "Test Accuracy : 0.9909\n",
      "Test Accuracy : 0.9907\n",
      "Test Accuracy : 0.9908\n",
      "Test Accuracy : 0.9907\n",
      "Test Accuracy : 0.9908\n",
      "Test Accuracy : 0.9909\n",
      "Test Accuracy : 0.9910\n",
      "Test Accuracy : 0.9911\n",
      "Test Accuracy : 0.9911\n",
      "Test Accuracy : 0.9912\n",
      "Test Accuracy : 0.9912\n",
      "Test Accuracy : 0.9912\n",
      "Test Accuracy : 0.9913\n",
      "Test Accuracy : 0.9914\n",
      "Test Accuracy : 0.9915\n",
      "Test Accuracy : 0.9915\n",
      "Test Accuracy : 0.9916\n",
      "Test Accuracy : 0.9917\n",
      "Test Accuracy : 0.9917\n",
      "Test Accuracy : 0.9918\n",
      "Test Accuracy : 0.9919\n",
      "Test Accuracy : 0.9919\n",
      "Test Accuracy : 0.9920\n",
      "Test Accuracy : 0.9919\n",
      "Test Accuracy : 0.9920\n",
      "Test Accuracy : 0.9921\n",
      "Test Accuracy : 0.9920\n",
      "Test Accuracy : 0.9921\n",
      "Test Accuracy : 0.9920\n",
      "Test Accuracy : 0.9920\n",
      "Test Accuracy : 0.9920\n",
      "Test Accuracy : 0.9918\n",
      "Test Accuracy : 0.9919\n",
      "Test Accuracy : 0.9920\n",
      "Test Accuracy : 0.9920\n",
      "Test Accuracy : 0.9921\n",
      "Test Accuracy : 0.9921\n",
      "Test Accuracy : 0.9922\n",
      "Test Accuracy : 0.9921\n",
      "Test Accuracy : 0.9922\n",
      "Test Accuracy : 0.9922\n",
      "Test Accuracy : 0.9923\n",
      "Test Accuracy : 0.9923\n",
      "Test Accuracy : 0.9924\n",
      "Test Accuracy : 0.9925\n",
      "Test Accuracy : 0.9925\n",
      "Test Accuracy : 0.9926\n",
      "Test Accuracy : 0.9926\n",
      "Test Accuracy : 0.9924\n",
      "Test Accuracy : 0.9924\n",
      "Test Accuracy : 0.9923\n",
      "Test Accuracy : 0.9922\n",
      "Test Accuracy : 0.9921\n",
      "Test Accuracy : 0.9921\n",
      "Test Accuracy : 0.9921\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        \n",
    "        print(f\"Test Accuracy : {accuracy:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.032, Test Accuracy: 0.992\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print('Test Loss: %.3f, Test Accuracy: %.3f' % (test_loss / len(test_dataloader), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. RESULTS ANALYSIS\n",
    "#### The test loass of 0.032 and Test accuracy of 0.992 (99.2%) shows that the DenseNET-121 model adjusted with parameters\n",
    "#### trained for 10 epochs at a learning rate of 0.01 in batches of 64 since the training was cpu bound. With GPU training can be faster\n",
    "#### size can be set to 128 with samme epochs and maybe training can be less that 7 hours. The DenseNet -121 is one of the best models\n",
    "#### for the MNIST dataset and model can be leveraged for datasets with the same characteristics for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#Initialise lists to store predictions and labels\n",
    "predictions = []\n",
    "labels = []\n",
    "results = []\n",
    "#Iterate over the test dataset\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs, batch_labels = batch\n",
    "        inputs, batch_labels = inputs.to(device),batch_labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        labels.extend(batch_labels.cpu().tolist())\n",
    "    for i in range(len(batch_labels)):\n",
    "        result = {'Truth Label': batch_labels[i].cpu().item(),'Predicted Label': predicted[i].cpu().tolist()}\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 973    0    2    0    0    1    2    2    0    0]\n",
      " [   0 1135    0    0    0    0    0    0    0    0]\n",
      " [   0    1 1029    0    0    0    0    2    0    0]\n",
      " [   0    0    0 1007    0    2    0    0    0    1]\n",
      " [   0    0    2    0  960    0    2    0    1   17]\n",
      " [   0    1    0    5    0  884    1    0    0    1]\n",
      " [   4    3    0    0    0    4  947    0    0    0]\n",
      " [   0    2    2    0    0    0    0 1024    0    0]\n",
      " [   0    0    4    2    0    3    0    1  962    2]\n",
      " [   1    1    0    1    1    3    0    2    0 1000]]\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n",
      "{'Truth Label': 6, 'Predicted Label': 6}\n"
     ]
    }
   ],
   "source": [
    "#Calculate the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(labels, predictions)\n",
    "print(conf_mat)\n",
    "for resullts in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(model.state_dict(), 'mnist_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
