{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Gradient:\n",
      " tensor([0.2350, 0.1966, 0.1050])\n",
      "Tanh Gradient:\n",
      " tensor([0.7864, 0.4200, 0.0707])\n",
      "ReLU Gradient:\n",
      " tensor([1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(44)\n",
    "# Define input tensor\n",
    "x_in = torch.tensor([0.5, -1.0, 2.0], requires_grad=True)\n",
    "error_grad = torch.tensor([1.0, 1.0, 1.0])  # Example gradient of loss w.r.t output\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid_out = torch.sigmoid(x_in)\n",
    "sigmoid_out.backward(error_grad, retain_graph=True)\n",
    "sigmoid_grad = x_in.grad.clone()\n",
    "x_in.grad.zero_()  # Reset gradient\n",
    "\n",
    "# Tanh\n",
    "tanh_out = torch.tanh(x_in)\n",
    "tanh_out.backward(error_grad, retain_graph=True)\n",
    "tanh_grad = x_in.grad.clone()\n",
    "x_in.grad.zero_()\n",
    "\n",
    "# ReLU\n",
    "relu_out = F.relu(x_in)\n",
    "relu_out.backward(error_grad, retain_graph=True)\n",
    "relu_grad = x_in.grad.clone()\n",
    "\n",
    "# Print results\n",
    "print(\"Sigmoid Gradient:\\n\", sigmoid_grad)\n",
    "print(\"Tanh Gradient:\\n\", tanh_grad)\n",
    "print(\"ReLU Gradient:\\n\", relu_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ### Example of the sgmoid activation function in respect of (∂E/∂χ_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (χ_in): tensor([ 0.5000, -1.0000,  2.0000], requires_grad=True)\n",
      "Sigmoid Output (χ_out): tensor([0.6225, 0.2689, 0.8808], grad_fn=<SigmoidBackward0>)\n",
      "Computed Gradient (∂E/∂χ_in): tensor([0.2350, 0.1966, 0.1050])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define input tensor\n",
    "x_in = torch.tensor([0.5, -1.0, 2.0], requires_grad=True)  # Input values\n",
    "error_grad = torch.tensor([1.0, 1.0, 1.0])  # Example gradient of loss w.r.t output\n",
    "\n",
    "# Apply sigmoid activation\n",
    "sigmoid_out = torch.sigmoid(x_in)\n",
    "\n",
    "# Compute gradients using backpropagation\n",
    "sigmoid_out.backward(error_grad, retain_graph=True)\n",
    "\n",
    "# Extract computed gradients\n",
    "sigmoid_grad = x_in.grad.clone()\n",
    "\n",
    "# Print results\n",
    "print(\"Input (χ_in):\", x_in)\n",
    "print(\"Sigmoid Output (χ_out):\", sigmoid_out)\n",
    "print(\"Computed Gradient (∂E/∂χ_in):\", sigmoid_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ### An example of Tanh activation function in respect of (∂E/∂χ_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (χ_in): tensor([ 0.5000, -1.0000,  2.0000], requires_grad=True)\n",
      "Tanh Output (χ_out): tensor([ 0.4621, -0.7616,  0.9640], grad_fn=<TanhBackward0>)\n",
      "Computed Gradient (∂E/∂χ_in): tensor([0.7864, 0.4200, 0.0707])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define an input tensor with requires_grad enabled to track gradients\n",
    "x_in = torch.tensor([0.5, -1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# Example gradient from the next layer (∂E/∂χ_out)\n",
    "error_grad = torch.tensor([1.0, 1.0, 1.0])\n",
    "\n",
    "# Apply tanh activation\n",
    "tanh_out = torch.tanh(x_in)\n",
    "\n",
    "# Perform backpropagation: compute ∂E/∂χ_in\n",
    "tanh_out.backward(error_grad)\n",
    "\n",
    "# The gradient computed is:\n",
    "# ∂E/∂χ_in = ∂E/∂χ_out * (1 - tanh(x_in)^2)\n",
    "print(\"Input (χ_in):\", x_in)\n",
    "print(\"Tanh Output (χ_out):\", tanh_out)\n",
    "print(\"Computed Gradient (∂E/∂χ_in):\", x_in.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ### An example of ReLU activation function in respect of (∂E/∂χ_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (χ_in): tensor([-1.,  0.,  2.], requires_grad=True)\n",
      "ReLU Output (χ_out): tensor([0., 0., 2.], grad_fn=<ReluBackward0>)\n",
      "Computed Gradient (∂E/∂χ_in): tensor([0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define an input tensor with requires_grad=True to track gradients\n",
    "x_in = torch.tensor([-1.0, 0.0, 2.0], requires_grad=True)\n",
    "\n",
    "# Example gradient from the next layer (∂E/∂χ_out)\n",
    "error_grad = torch.tensor([1.0, 1.0, 1.0])  # Assume error is propagated equally\n",
    "\n",
    "# Apply ReLU activation\n",
    "relu_out = torch.relu(x_in)\n",
    "\n",
    "# Perform backpropagation\n",
    "relu_out.backward(error_grad)\n",
    "\n",
    "# The gradient computed is:\n",
    "# ∂E/∂χ_in = ∂E/∂χ_out * ReLU'(χ_in)\n",
    "print(\"Input (χ_in):\", x_in)\n",
    "print(\"ReLU Output (χ_out):\", relu_out)\n",
    "print(\"Computed Gradient (∂E/∂χ_in):\", x_in.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
